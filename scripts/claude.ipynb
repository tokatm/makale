{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "687546b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725411bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pin memory: True | Workers: 11 | AMP: True\n",
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/tmp/ipython-input-3274657443.py:86: UserWarning: Argument(s) 'pad_mode, pad_val, mask_pad_val' are not valid for transform Perspective\n",
      "  A.Perspective(\n",
      "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset-2/images/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3274657443.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-3274657443.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading datasets...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m     train_dataset = PotholeDataset(\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_img_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_label_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3274657443.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, label_dir, transforms)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         self.images = [f for f in os.listdir(image_dir) \n\u001b[0m\u001b[1;32m    137\u001b[0m                       if f.endswith(('.jpg', '.jpeg', '.png'))]\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset-2/images/train'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn,\n",
    "    RetinaNet_ResNet50_FPN_Weights,\n",
    "    retinanet_resnet50_fpn,\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "if 'ipykernel' in sys.modules:\n",
    "    sys.argv = [sys.argv[0]]\n",
    "\n",
    "\n",
    "def default_num_workers():\n",
    "    cpu_count = os.cpu_count() or 0\n",
    "    return max(cpu_count - 1, 0)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 70\n",
    "    learning_rate: float = 0.005\n",
    "    num_classes: int = 2\n",
    "    conf_threshold: float = 0.25\n",
    "    model_name: str = 'fasterrcnn'\n",
    "    backbone: str = 'resnet50'\n",
    "    num_workers: int = default_num_workers()\n",
    "    train_img_dir: str = os.path.join('dataset-2', 'images', 'train')\n",
    "    train_label_dir: str = os.path.join('dataset-2', 'labels', 'train')\n",
    "    val_img_dir: str = os.path.join('dataset-2', 'images', 'val')\n",
    "    val_label_dir: str = os.path.join('dataset-2', 'labels', 'val')\n",
    "    output_dir: str = 'results'\n",
    "    seed: int = 42\n",
    "    use_amp: bool = True\n",
    "    pretrained: bool = True\n",
    "    warmup_epochs: int = 5\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def get_augmentation_pipeline(train=True):\n",
    "    \"\"\"\n",
    "    Albumentations pipeline'ını oluşturur.\n",
    "    ToTensorV2 ve Normalize dahildir.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            # Augmentations\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.4, \n",
    "                contrast_limit=0.4, \n",
    "                brightness_by_max=True,\n",
    "                p=0.7),\n",
    "            A.GaussNoise(p=0.2),\n",
    "            A.Blur(blur_limit=3, p=0.2),\n",
    "            A.RandomRotate90(p=0.3),\n",
    "            A.Affine(\n",
    "                scale=(0.8, 1.2),\n",
    "                translate_percent=(-0.1, 0.1),\n",
    "                rotate=(-10, 10),\n",
    "                shear=(-5, 5),\n",
    "                p=0.4\n",
    "            ),\n",
    "            A.Perspective(\n",
    "                scale=(0.05, 0.1),\n",
    "                keep_size=True,\n",
    "                pad_mode=0,\n",
    "                pad_val=0,\n",
    "                mask_pad_val=0,\n",
    "                p=0.3\n",
    "            ),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, \n",
    "                              rotate_limit=10, p=0.3),\n",
    "            # Normalization ve ToTensorV2\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            A.ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(\n",
    "            format='yolo', \n",
    "            label_fields=['class_labels'],\n",
    "            min_area=0,\n",
    "            min_visibility=0.3,\n",
    "            clip=True\n",
    "        ))\n",
    "    else:\n",
    "        # Validation için sadece normalization ve ToTensorV2\n",
    "        transform = A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            A.ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(\n",
    "            format='yolo', \n",
    "            label_fields=['class_labels'],\n",
    "            min_area=0,\n",
    "            min_visibility=0.3,\n",
    "            clip=True\n",
    "        ))\n",
    "    \n",
    "    return transform\n",
    "\n",
    "\n",
    "class PotholeDataset(Dataset):\n",
    "    \"\"\"YOLO formatındaki çukur verilerini PyTorch object detection formatına dönüştürür\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images = [f for f in os.listdir(image_dir) \n",
    "                      if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def yolo_to_pascal_voc(self, yolo_boxes, img_width, img_height):\n",
    "        \"\"\"YOLO formatını Pascal VOC formatına dönüştür\"\"\"\n",
    "        pascal_boxes = []\n",
    "        for box in yolo_boxes:\n",
    "            x_center, y_center, width, height = box\n",
    "            \n",
    "            x_min = (x_center - width / 2) * img_width\n",
    "            y_min = (y_center - height / 2) * img_height\n",
    "            x_max = (x_center + width / 2) * img_width\n",
    "            y_max = (y_center + height / 2) * img_height\n",
    "            \n",
    "            pascal_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        \n",
    "        return pascal_boxes\n",
    "    \n",
    "    def clip_pascal_boxes(self, boxes, img_width, img_height):\n",
    "        \"\"\"Pascal VOC kutularını görüntü boyutuna sınırlar ve geçersizleri atar\"\"\"\n",
    "        clipped = []\n",
    "        for x_min, y_min, x_max, y_max in boxes:\n",
    "            x_min = np.clip(x_min, 0, img_width)\n",
    "            y_min = np.clip(y_min, 0, img_height)\n",
    "            x_max = np.clip(x_max, 0, img_width)\n",
    "            y_max = np.clip(y_max, 0, img_height)\n",
    "            if x_max > x_min and y_max > y_min:\n",
    "                clipped.append([x_min, y_min, x_max, y_max])\n",
    "        return clipped\n",
    "    \n",
    "    def clip_boxes(self, boxes):\n",
    "        \"\"\"Bbox koordinatlarını [0, 1] aralığına sınırla\"\"\"\n",
    "        clipped_boxes = []\n",
    "        for box in boxes:\n",
    "            x_center = np.clip(box[0], 0.0, 1.0)\n",
    "            y_center = np.clip(box[1], 0.0, 1.0)\n",
    "            width = np.clip(box[2], 0.0, 1.0)\n",
    "            height = np.clip(box[3], 0.0, 1.0)\n",
    "            \n",
    "            x_center = np.clip(x_center, width/2, 1.0 - width/2)\n",
    "            y_center = np.clip(y_center, height/2, 1.0 - height/2)\n",
    "            \n",
    "            clipped_boxes.append([x_center, y_center, width, height])\n",
    "        return clipped_boxes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if os.path.exists(label_path) and os.path.getsize(label_path) > 0:\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        if class_id == 0:\n",
    "                            x_center, y_center, width, height = map(float, parts[1:])\n",
    "                            x_center = np.clip(x_center, 0.0, 1.0)\n",
    "                            y_center = np.clip(y_center, 0.0, 1.0)\n",
    "                            width = np.clip(width, 0.0, 1.0)\n",
    "                            height = np.clip(height, 0.0, 1.0)\n",
    "                            boxes.append([x_center, y_center, width, height])\n",
    "                            labels.append(1)\n",
    "        \n",
    "        # Albumentations transformunu uygula\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                class_labels=labels\n",
    "            )\n",
    "            image = transformed['image']  # ToTensorV2 ile tensor'e dönüştü\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['class_labels']\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            boxes = self.yolo_to_pascal_voc(boxes, w, h)\n",
    "            boxes = self.clip_pascal_boxes(boxes, w, h)\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if len(boxes) > 0 else torch.zeros((0,), dtype=torch.float32),\n",
    "            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def debug_sample(dataset, idx=0, save_path=None):\n",
    "    \"\"\"Bir örneği görselleştir\"\"\"\n",
    "    image, target = dataset[idx]\n",
    "    \n",
    "    # Tensor'ı numpy'a çevir (ToTensorV2 sonrası)\n",
    "    if torch.is_tensor(image):\n",
    "        # Inverse normalize: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        image_np = image.numpy().transpose(1, 2, 0)\n",
    "        image_np = image_np * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
    "        image_np = np.clip(image_np, 0, 1)\n",
    "    else:\n",
    "        image_np = image\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image_np)\n",
    "    \n",
    "    boxes = target['boxes'].cpu().numpy() if torch.is_tensor(target['boxes']) else target['boxes']\n",
    "    labels = target['labels'].cpu().numpy() if torch.is_tensor(target['labels']) else target['labels']\n",
    "    \n",
    "    print(f\"Image shape: {image_np.shape}\")\n",
    "    print(f\"Number of boxes: {len(boxes)}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), w, h, \n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(x1, y1-5, f'Pothole ({label})', \n",
    "                color='white', fontsize=10, \n",
    "                bbox=dict(facecolor='red', alpha=0.8))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=100)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"DataLoader için custom collate function\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def clean_batch(images, targets, device):\n",
    "    \"\"\"Kutuları doğrula, ters/NaN kutuları at ve tensörleri cihaza taşı\"\"\"\n",
    "    processed_images = []\n",
    "    processed_targets = []\n",
    "    \n",
    "    for img, tgt in zip(images, targets):\n",
    "        img = img.to(device)\n",
    "        tgt = {k: v.to(device) if hasattr(v, 'to') else v for k, v in tgt.items()}\n",
    "        boxes = tgt['boxes']\n",
    "        \n",
    "        if boxes.numel() > 0:\n",
    "            finite_mask = torch.isfinite(boxes).all(dim=1)\n",
    "            size_mask = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "            keep = finite_mask & size_mask\n",
    "            \n",
    "            if keep.any():\n",
    "                tgt = tgt.copy()\n",
    "                tgt['boxes'] = boxes[keep]\n",
    "                if len(tgt['labels']) == len(boxes):\n",
    "                    tgt['labels'] = tgt['labels'][keep]\n",
    "                tgt['area'] = (tgt['boxes'][:, 3] - tgt['boxes'][:, 1]) * (tgt['boxes'][:, 2] - tgt['boxes'][:, 0])\n",
    "            else:\n",
    "                tgt = tgt.copy()\n",
    "                tgt['boxes'] = boxes.new_zeros((0, 4))\n",
    "                tgt['labels'] = torch.zeros((0,), dtype=torch.int64, device=boxes.device)\n",
    "                tgt['area'] = boxes.new_zeros((0,))\n",
    "        else:\n",
    "            tgt = tgt.copy()\n",
    "            tgt['boxes'] = boxes\n",
    "            tgt['area'] = boxes.new_zeros((0,)) if hasattr(boxes, 'new_zeros') else torch.zeros((0,), device=device)\n",
    "        \n",
    "        processed_images.append(img)\n",
    "        processed_targets.append(tgt)\n",
    "    \n",
    "    return processed_images, processed_targets\n",
    "\n",
    "\n",
    "def create_faster_rcnn_optimized(num_classes=2, pretrained=True, img_size=640):\n",
    "    \"\"\"\n",
    "    Çukur tespiti için optimize edilmiş Faster R-CNN\n",
    "    \"\"\"\n",
    "    from torchvision.models.detection import (\n",
    "        FasterRCNN_ResNet50_FPN_Weights,\n",
    "        fasterrcnn_resnet50_fpn\n",
    "    )\n",
    "    from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "    from torchvision.models.detection.rpn import RPNHead\n",
    "\n",
    "    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT if pretrained else None\n",
    "    model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    anchor_sizes = (\n",
    "        (8, 12, 16),\n",
    "        (24, 32, 48),\n",
    "        (64, 96, 128),\n",
    "        (192, 256, 320),\n",
    "        (384, 512, 768),\n",
    "    )\n",
    "    aspect_ratios = ((0.5, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n",
    "\n",
    "    model.rpn.anchor_generator = AnchorGenerator(\n",
    "        sizes=anchor_sizes,\n",
    "        aspect_ratios=aspect_ratios\n",
    "    )\n",
    "\n",
    "    in_channels = model.backbone.out_channels\n",
    "    num_anchors_per_location = model.rpn.anchor_generator.num_anchors_per_location()\n",
    "    num_anchors = num_anchors_per_location[0]\n",
    "\n",
    "    model.rpn.head = RPNHead(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=num_anchors\n",
    "    )\n",
    "\n",
    "    model.rpn.fg_iou_thresh = 0.5\n",
    "    model.rpn.bg_iou_thresh = 0.3\n",
    "    model.rpn.batch_size_per_image = 512\n",
    "    model.rpn.positive_fraction = 0.5\n",
    "\n",
    "    model.rpn.nms_thresh = 0.7\n",
    "    model.rpn.score_thresh = 0.0\n",
    "    model.rpn.pre_nms_top_n_train = 2000\n",
    "    model.rpn.post_nms_top_n_train = 2000\n",
    "    model.rpn.pre_nms_top_n_test = 1000\n",
    "    model.rpn.post_nms_top_n_test = 1000\n",
    "\n",
    "    model.roi_heads.fg_iou_thresh = 0.5\n",
    "    model.roi_heads.bg_iou_thresh = 0.5\n",
    "    model.roi_heads.batch_size_per_image = 512\n",
    "    model.roi_heads.positive_fraction = 0.5\n",
    "    model.roi_heads.nms_thresh = 0.4\n",
    "    model.roi_heads.score_thresh = 0.05\n",
    "    model.roi_heads.detections_per_img = 300\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ FASTER R-CNN ÇUKUR-SPESİFİK KONFİGÜRASYON\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Anchor Sizes: {anchor_sizes}\")\n",
    "    print(f\"Aspect Ratios: {aspect_ratios[0]}\")\n",
    "    print(f\"RPN Input Channels: {in_channels}\")\n",
    "    print(f\"Anchors per Location: {num_anchors}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, scaler=None, warmup_epochs=5):\n",
    "    \"\"\"Warmup ve gradient clipping ile geliştirilmiş eğitim\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_steps = 0\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=f'Epoch {epoch}')\n",
    "    for batch_idx, (images, targets) in enumerate(pbar):\n",
    "        if epoch <= warmup_epochs:\n",
    "            warmup_factor = min(1.0, (batch_idx + 1) / len(data_loader))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                base_lr = param_group.get('initial_lr', param_group['lr'])\n",
    "                param_group['lr'] = base_lr * warmup_factor\n",
    "        \n",
    "        use_amp = scaler is not None and scaler.is_enabled()\n",
    "        images, targets = clean_batch(images, targets, device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "            if 'loss_classifier' in loss_dict:\n",
    "                losses = loss_dict['loss_classifier'] * 2.0 + \\\n",
    "                        loss_dict['loss_box_reg'] * 1.0 + \\\n",
    "                        loss_dict['loss_objectness'] * 1.0 + \\\n",
    "                        loss_dict['loss_rpn_box_reg'] * 1.0\n",
    "            else:\n",
    "                losses = loss_dict.get('classification', 0) + loss_dict.get('bbox_regression', 0)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        num_steps += 1\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{losses.item():.4f}',\n",
    "            'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / max(num_steps, 1)\n",
    "\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"IoU (Intersection over Union) hesapla\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device, iou_threshold=0.5, conf_threshold=0.25):\n",
    "    \"\"\"Model performansını değerlendir ve bilimsel metrikler hesapla\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    total_iou = 0\n",
    "    iou_count = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc='Evaluating')):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "\n",
    "                if len(pred_boxes) > 0:\n",
    "                    import torchvision.ops as ops\n",
    "                    nms_indices = ops.nms(\n",
    "                        torch.tensor(pred_boxes),\n",
    "                        torch.tensor(pred_scores),\n",
    "                        iou_threshold=0.3\n",
    "                    )\n",
    "                    pred_boxes = pred_boxes[nms_indices]\n",
    "                    pred_scores = pred_scores[nms_indices]\n",
    "                \n",
    "                mask = pred_scores >= conf_threshold\n",
    "                pred_boxes = pred_boxes[mask]\n",
    "                pred_scores = pred_scores[mask]\n",
    "                \n",
    "                gt_boxes = target['boxes'].cpu().numpy()\n",
    "                \n",
    "                if len(gt_boxes) == 0:\n",
    "                    fp += len(pred_boxes)\n",
    "                    continue\n",
    "                \n",
    "                if len(pred_boxes) == 0:\n",
    "                    fn += len(gt_boxes)\n",
    "                    continue\n",
    "                \n",
    "                matched_pred = set()\n",
    "                for gt_box in gt_boxes:\n",
    "                    best_iou = 0\n",
    "                    best_pred_idx = -1\n",
    "                    \n",
    "                    for idx, pred_box in enumerate(pred_boxes):\n",
    "                        if idx in matched_pred:\n",
    "                            continue\n",
    "                        \n",
    "                        iou = calculate_iou(gt_box, pred_box)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_pred_idx = idx\n",
    "                    \n",
    "                    if best_iou >= iou_threshold:\n",
    "                        tp += 1\n",
    "                        matched_pred.add(best_pred_idx)\n",
    "                        total_iou += best_iou\n",
    "                        iou_count += 1\n",
    "                    else:\n",
    "                        fn += 1\n",
    "                \n",
    "                fp += len([i for i in range(len(pred_boxes)) if i not in matched_pred])\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    mean_iou = total_iou / iou_count if iou_count > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score,\n",
    "        'mAP@0.5': precision,\n",
    "        'Mean IoU': mean_iou,\n",
    "        'True Positives': tp,\n",
    "        'False Positives': fp,\n",
    "        'False Negatives': fn,\n",
    "        'Confidence Threshold': conf_threshold\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_metrics_for_paper(metrics):\n",
    "    \"\"\"Bilimsel makale için metrik çıktısı\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BİLİMSEL MAKALE İÇİN PERFORMANS METRİKLERİ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPrecision (Kesinlik):        {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall (Duyarlılık):         {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1-Score:                    {metrics['F1-Score']:.4f}\")\n",
    "    print(f\"mAP@0.5:                     {metrics['mAP@0.5']:.4f}\")\n",
    "    print(f\"Mean IoU:                    {metrics['Mean IoU']:.4f}\")\n",
    "    print(f\"Confidence Threshold:        {metrics['Confidence Threshold']:.2f}\")\n",
    "    print(f\"\\nTrue Positives (TP):         {metrics['True Positives']}\")\n",
    "    print(f\"False Positives (FP):        {metrics['False Positives']}\")\n",
    "    print(f\"False Negatives (FN):        {metrics['False Negatives']}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Pothole tespiti için Faster R-CNN eğitimi')\n",
    "    parser.add_argument('--train-img-dir', type=str, default=None)\n",
    "    parser.add_argument('--train-label-dir', type=str, default=None)\n",
    "    parser.add_argument('--val-img-dir', type=str, default=None)\n",
    "    parser.add_argument('--val-label-dir', type=str, default=None)\n",
    "    parser.add_argument('--batch-size', type=int, default=None)\n",
    "    parser.add_argument('--epochs', type=int, default=None)\n",
    "    parser.add_argument('--lr', type=float, default=None)\n",
    "    parser.add_argument('--conf-threshold', type=float, default=None)\n",
    "    parser.add_argument('--num-workers', type=int, default=None)\n",
    "    parser.add_argument('--output-dir', type=str, default=None)\n",
    "    parser.add_argument('--seed', type=int, default=None)\n",
    "    parser.add_argument('--no-amp', action='store_true')\n",
    "    parser.add_argument('--no-pretrained', action='store_true')\n",
    "    parser.add_argument('--model', type=str, default=None, choices=['retinanet', 'fasterrcnn'])\n",
    "    parser.add_argument('--device', type=str, default=None)\n",
    "    parser.add_argument('--img-size', type=int, default=640, help='Görüntü boyutu (anchor hesaplaması için)')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    config = TrainConfig()\n",
    "    \n",
    "    if args.batch_size:\n",
    "        config.batch_size = args.batch_size\n",
    "    if args.epochs:\n",
    "        config.num_epochs = args.epochs\n",
    "    if args.lr:\n",
    "        config.learning_rate = args.lr\n",
    "    if args.conf_threshold is not None:\n",
    "        config.conf_threshold = args.conf_threshold\n",
    "    if args.num_workers is not None:\n",
    "        config.num_workers = args.num_workers\n",
    "    if args.train_img_dir:\n",
    "        config.train_img_dir = args.train_img_dir\n",
    "    if args.train_label_dir:\n",
    "        config.train_label_dir = args.train_label_dir\n",
    "    if args.val_img_dir:\n",
    "        config.val_img_dir = args.val_img_dir\n",
    "    if args.val_label_dir:\n",
    "        config.val_label_dir = args.val_label_dir\n",
    "    if args.output_dir:\n",
    "        config.output_dir = args.output_dir\n",
    "    if args.seed is not None:\n",
    "        config.seed = args.seed\n",
    "    if args.no_amp:\n",
    "        config.use_amp = False\n",
    "    if args.no_pretrained:\n",
    "        config.pretrained = False\n",
    "    if args.model:\n",
    "        config.model_name = args.model\n",
    "    \n",
    "    device = torch.device(\n",
    "        args.device if args.device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    )\n",
    "    pin_memory = device.type == 'cuda'\n",
    "    \n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    seed_everything(config.seed)\n",
    "    if hasattr(torch, 'set_float32_matmul_precision'):\n",
    "        torch.set_float32_matmul_precision('medium')\n",
    "    \n",
    "    amp_active = config.use_amp and device.type == \"cuda\"\n",
    "    print(f'Using device: {device}')\n",
    "    print(f'Pin memory: {pin_memory} | Workers: {config.num_workers} | AMP: {amp_active}')\n",
    "    \n",
    "    print('Loading datasets...')\n",
    "    train_dataset = PotholeDataset(\n",
    "        config.train_img_dir, \n",
    "        config.train_label_dir,\n",
    "        transforms=get_augmentation_pipeline(train=True)\n",
    "    )\n",
    "    \n",
    "    val_dataset = PotholeDataset(\n",
    "        config.val_img_dir, \n",
    "        config.val_label_dir,\n",
    "        transforms=get_augmentation_pipeline(train=False)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=config.num_workers > 0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=config.num_workers > 0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Debug için\n",
    "    debug_sample(train_dataset, idx=0)\n",
    "    debug_sample(val_dataset, idx=10)\n",
    "    \n",
    "    print(f'Train samples: {len(train_dataset)}')\n",
    "    print(f'Val samples: {len(val_dataset)}')\n",
    "    \n",
    "    print('\\nCreating model...')\n",
    "    img_size = args.img_size if args else 640\n",
    "    model = create_faster_rcnn_optimized(\n",
    "        num_classes=config.num_classes,\n",
    "        pretrained=config.pretrained,\n",
    "        img_size=img_size\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, \n",
    "        lr=config.learning_rate,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['initial_lr'] = param_group['lr']\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=[35, 55],\n",
    "        gamma=0.1\n",
    "    )\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.use_amp and device.type == 'cuda')\n",
    "    \n",
    "    print('\\nStarting training...')\n",
    "    print(f'Initial Confidence Threshold: {config.conf_threshold}')\n",
    "    best_f1 = -float('inf')\n",
    "    best_model_path = os.path.join(config.output_dir, 'best_pothole_model.pth')\n",
    "    metrics_path = os.path.join(config.output_dir, 'metrics.json')\n",
    "    \n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        train_loss = train_one_epoch(\n",
    "            model, optimizer, train_loader, device, \n",
    "            epoch, scaler=scaler, warmup_epochs=config.warmup_epochs\n",
    "        )\n",
    "        \n",
    "        if epoch > config.warmup_epochs:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch}/{config.num_epochs} - train loss: {train_loss:.4f} - LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        if epoch % 3 == 0 or epoch == config.num_epochs:\n",
    "            print(f'\\nValidating epoch {epoch}...')\n",
    "            metrics = evaluate_model(model, val_loader, device, conf_threshold=config.conf_threshold)\n",
    "            print_metrics_for_paper(metrics)\n",
    "            \n",
    "            if metrics['F1-Score'] > best_f1:\n",
    "                best_f1 = metrics['F1-Score']\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f'✓ Best model saved! F1-Score: {best_f1:.4f}')\n",
    "    \n",
    "    print('\\n\\nFINAL EVALUATION:')\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        final_metrics = evaluate_model(model, val_loader, device, conf_threshold=config.conf_threshold)\n",
    "        print_metrics_for_paper(final_metrics)\n",
    "        \n",
    "        final_metrics_json = {}\n",
    "        for k, v in final_metrics.items():\n",
    "            if isinstance(v, (np.floating,)):\n",
    "                final_metrics_json[k] = float(v)\n",
    "            elif isinstance(v, (np.integer,)):\n",
    "                final_metrics_json[k] = int(v)\n",
    "            elif hasattr(v, 'item'):\n",
    "                final_metrics_json[k] = v.item()\n",
    "            else:\n",
    "                final_metrics_json[k] = v\n",
    "        \n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(final_metrics_json, f, indent=4)\n",
    "        \n",
    "        print('\\n✓ Training completed!')\n",
    "        print(f'✓ Model saved: {best_model_path}')\n",
    "        print(f'✓ Metrics saved: {metrics_path}')\n",
    "    else:\n",
    "        print('Best model bulunamadı!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d0c6bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/content/scripts/claude.ipynb': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python scripts/claude.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df69d522",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3146998594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive not found. Mount Google Drive first (run the cell that calls drive.mount).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "target = os.path.join(base, 'Dataset')\n",
    "\n",
    "if not os.path.exists('/content/drive'):\n",
    "    raise FileNotFoundError(\"/content/drive not found. Mount Google Drive first (run the cell that calls drive.mount).\")\n",
    "\n",
    "# show what's in MyDrive if it exists\n",
    "if os.path.exists(base):\n",
    "    print(\"Contents of MyDrive:\", os.listdir(base))\n",
    "else:\n",
    "    print(f\"{base} does not exist. Creating it.\")\n",
    "    os.makedirs(base, exist_ok=True)\n",
    "\n",
    "# create Dataset if missing and change directory\n",
    "if not os.path.exists(target):\n",
    "    print(f\"Creating '{target}'...\")\n",
    "    os.makedirs(target, exist_ok=True)\n",
    "\n",
    "os.chdir(target)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Directory contents:\", os.listdir('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
